<!DOCTYPE html>
<html 
	lang="zh-CN">
	<head>
		<meta charset="UTF-8" />
		<meta http-equiv="X-UA-Compatible" content="IE=edge" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		
<link rel="stylesheet" href="/css/layout.css">

		
		<title> 草履虫的transformer初步理解 -  KK空间</title>
		<!-- <link rel="stylesheet" href="https://unpkg.com/mdui@1.0.2/dist/css/mdui.min.css" /> -->
		<!-- <script src="https://unpkg.com/mdui@1.0.2/dist/js/mdui.min.js"></script> -->
		
<link rel="stylesheet" href="/lib/mdui/mdui.min.css">

		
<script src="/lib/mdui/mdui.min.js"></script>

		<!-- lazyload -->
		
<script src="/lib/lazysizes.js"></script>

		<!-- smooth-scrolling -->
		
<script src="/lib/smooth-scrolling.js"></script>

		<!-- highlight -->
		
<link rel="stylesheet" href="/lib/highlight/atom-one-dark.min.css">

		
<script src="/lib/highlight/highlight.min.js"></script>

		<!-- 预置 kiraicon -->
		
<link rel="stylesheet" href="/lib/iconfont/iconfont.css">

		
		<link
			rel="shortcut icon"
			href="/image/fa.jpeg"
			type="image/jpeg"
		/>
		
<link rel="stylesheet" href="/deps/css/APlayer.min.css">

		
		
<script src="/deps/js/APlayer.min.js"></script>
<script src="/deps/js/Meting.min.js"></script>

	<!-- hexo injector head_end start -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$']],
          displayMath: [['$$', '$$']],
          processEscapes: true
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
    <!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>

	<body>
		<div
			class="kira-background"
			style="background-image: url('/image/frieren.png')"
		></div>
		<div class="kira-header">
    <a
        class="kira-drawer-button mdui-ripple"
        title="导航栏"
        onclick="document.querySelector('.kira-sidebar-modal').classList.add('show');document.querySelector('.kira-sidebar#sidebar').classList.add('show');"
    >
        <i class="kirafont icon-menu"></i>
    </a>
    <a href="/" title="KK空间">
        <img
			src="/image/chongye.png"
			alt="战斗包子"
		/>
    </a>
</div>
		<div class="kira-body">
			<div class="kira-sidebar" id="sidebar">
	<div class="kira-avatar mdui-ripple">
		<a href="/image/chongye.png" title="战斗包子">
			<img
				src="/image/chongye.png"
				alt="战斗包子"
			/>
		</a>
	</div>
	<div class="kira-count">
		<div><span>文章</span>124</div>
		<div><span>标签</span>24</div>
		<div><span>分类</span>0</div>
	</div>
	<div class="kira-list">
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/"
			title="回到首页"
		>
			<i
				class="kirafont
					
						icon-home
					"
			></i>
			<div class="kira-list-item-content">
				回到首页
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/archive.html"
			title="文章归档"
		>
			<i
				class="kirafont
					
						icon-container
					"
			></i>
			<div class="kira-list-item-content">
				文章归档
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/about.html"
			title="关于本人"
		>
			<i
				class="kirafont
					
						icon-user
					"
			></i>
			<div class="kira-list-item-content">
				关于本人
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/friends.html"
			title="我的朋友"
		>
			<i
				class="kirafont
					
						icon-team
					"
			></i>
			<div class="kira-list-item-content">
				我的朋友
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/todolist.html"
			title="我的Todo"
		>
			<i
				class="kirafont
					
						icon-container-fill
					"
			></i>
			<div class="kira-list-item-content">
				我的Todo
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/liferecords"
			title="玩了什么"
		>
			<i
				class="kirafont
					
						icon-fullscreen
					"
			></i>
			<div class="kira-list-item-content">
				玩了什么
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/game_graph.html"
			title="小图"
		>
			<i
				class="kirafont
					
						icon-fullscreen
					"
			></i>
			<div class="kira-list-item-content">
				小图
			</div>
		</a>
		
	</div>
	<aside id="kira-sidebar">
		
			<div class="kira-widget-wrap">
	<div class="kira-widget kira-social">
		
			<a
				class="mdui-ripple"
				href="tencent://AddContact/?fromId=45&fromSubId=1&subcmd=all&uin=1040035659&website=www.oicqzone.com"
				target="_blank"
				mdui-tooltip="{content: 'QQ'}"
				style="color: rgb(49, 174, 255); background-color: rgba(49, 174, 255, .1);"
			>
				<i
					class="kirafont
					
						icon-QQ
					"
				></i>
			</a>
		
			<a
				class="mdui-ripple"
				href="https://space.bilibili.com/6456506"
				target="_blank"
				mdui-tooltip="{content: '哔哩哔哩'}"
				style="color: rgb(231, 106, 141); background-color: rgba(231, 106, 141, .15);"
			>
				<i
					class="kirafont
					
						icon-bilibili
					"
				></i>
			</a>
		
			<a
				class="mdui-ripple"
				href="https://github.com/PaiPai121/"
				target="_blank"
				mdui-tooltip="{content: 'GitHub'}"
				style="color: rgb(25, 23, 23); background-color: rgba(25, 23, 23, .15);"
			>
				<i
					class="kirafont
					
						icon-github
					"
				></i>
			</a>
		
			<a
				class="mdui-ripple"
				href="https://gitee.com/<你的gitee id>"
				target="_blank"
				mdui-tooltip="{content: 'Gitee'}"
				style="color: rgb(165, 15, 15); background-color: rgba(165, 15, 15, .15);"
			>
				<i
					class="kirafont
					
						icon-gitee
					"
				></i>
			</a>
		
	</div>
</div>

		
			
		
			
	<div class="kira-widget-wrap">
		<div id="randomtagcloud" class="kira-widget tagcloud kira-rainbow">
			<a href="/tags/AI/" style="font-size: 11.11px;">AI</a> <a href="/tags/GameExtend/" style="font-size: 14.44px;">GameExtend</a> <a href="/tags/MMD/" style="font-size: 11.11px;">MMD</a> <a href="/tags/flash/" style="font-size: 11.11px;">flash</a> <a href="/tags/gaea%E6%A1%86%E6%9E%B6/" style="font-size: 13.33px;">gaea框架</a> <a href="/tags/travel/" style="font-size: 10px;">travel</a> <a href="/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/" style="font-size: 18.89px;">公众号</a> <a href="/tags/%E5%87%B8%E4%BC%98%E5%8C%96/" style="font-size: 11.11px;">凸优化</a> <a href="/tags/%E5%A6%99%E7%93%A6%E5%BA%95/" style="font-size: 10px;">妙瓦底</a> <a href="/tags/%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">学习</a> <a href="/tags/%E5%B7%A5%E4%BD%9C/" style="font-size: 10px;">工作</a> <a href="/tags/%E5%BC%80%E5%8F%91/" style="font-size: 12.22px;">开发</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 12.22px;">强化学习</a> <a href="/tags/%E6%80%80%E6%97%A7/" style="font-size: 11.11px;">怀旧</a> <a href="/tags/%E6%88%91%E7%9A%84%E8%AE%BA%E6%96%87/" style="font-size: 10px;">我的论文</a> <a href="/tags/%E6%97%A5%E5%B8%B8/" style="font-size: 17.78px;">日常</a> <a href="/tags/%E6%9C%AC%E5%9C%B0%E5%AD%98%E6%A1%A3/" style="font-size: 18.89px;">本地存档</a> <a href="/tags/%E6%B8%B8%E6%88%8F%E6%9D%82%E8%B0%88/" style="font-size: 16.67px;">游戏杂谈</a> <a href="/tags/%E6%B8%B8%E6%88%8F%E8%A1%8D%E7%94%9F/" style="font-size: 10px;">游戏衍生</a> <a href="/tags/%E7%9C%8B%E7%95%AA/" style="font-size: 11.11px;">看番</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/" style="font-size: 11.11px;">编程基本知识</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" style="font-size: 10px;">自动驾驶</a> <a href="/tags/%E8%8D%89%E5%B1%A5%E8%99%AB%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF/" style="font-size: 11.11px;">草履虫的端到端</a> <a href="/tags/%E9%9D%A2%E8%AF%95/" style="font-size: 15.56px;">面试</a>
		</div>
		
	</div>


		
			
	<div class="kira-widget-wrap">
		<h3 class="kira-widget-title">
			文章归档
		</h3>
		<div class="kira-widget">
			<ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2026/">2026</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/">2025</a><span class="archive-list-count">62</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/">2024</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">13</span></li></ul>
		</div>
	</div>


		
	</aside>
	<div class="kira-copyright">
		&copy; 2026
		<a href="/">战斗包子</a>
		Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> &
		<a href="https://github.com/ch1ny/kira-hexo/" target="_blank">Kira-Hexo</a>
		<br />
		
		
	</div>
</div>
<div
	class="kira-sidebar-modal"
	id="sidebar-modal"
	onclick="(function(self) {
		self.classList.remove('show');
		document.querySelector('.kira-sidebar.show#sidebar').classList.remove('show');
	})(this)"
></div>
			<div class="kira-content">
				<div id="kira-top-header"></div>
				<div class="kira-main-content">
					
<link rel="stylesheet" href="/css/kira-image.css">


<script src="/js/kira-image.js"></script>

<div class="kira-image">
    <div class="kira-image-modal">
        <div class="kira-image-header">
            <div class="kira-image-counter"></div>
            <div class="kira-image-title"></div>
            <div class="kira-image-operation">
                <div class="kira-image-operation-button" id="kira-image-operation-button-zoom">
                    <i class="kirafont icon-zoom-in"></i>
                </div>
                <div class="kira-image-operation-button" id="kira-image-operation-button-close">
                    <i class="kirafont icon-close"></i>
                </div>
            </div>
        </div>
        <div class="kira-image-container">
            <div class="kira-image-prev-button-panel">
                <div class="kira-image-exchange-button">
                    <i class="kirafont icon-left"></i>
                </div>
            </div>
            <div class="kira-image-list">
                <div class="kira-image-prev">
                    <img />
                </div>
                <div class="kira-image-now">
                    <img />
                </div>
                <div class="kira-image-next">
                    <img />
                </div>
            </div>
            <div class="kira-image-next-button-panel">
                <div class="kira-image-exchange-button">
                    <i class="kirafont icon-right"></i>
                </div>
            </div>
        </div>
    </div>
</div>

	
<link rel="stylesheet" href="/css/kira-code-copy.css">

	
<script src="/js/kira-code-copy.js"></script>


<div class="kira-post">
	<article>
		
		<div
			class="kira-post-cover"
			style="padding-bottom: '56.25%'"
		>
			<img
				data-src="/image/frieren.png"
				data-sizes="auto"
				alt="草履虫的transformer初步理解"
				class="lazyload kira-post-cover-image disabled-kira-image"
			/>
			<h1>草履虫的transformer初步理解</h1>
		</div>
		
		<div class="kira-post-meta kira-rainbow" style="margin:10px 0!important;">
			<a><i class="kirafont icon-calendar-fill"></i>2025年11月23日</a>
			<a><i class="kirafont icon-edit-fill"></i>2.6k 字</a>
			<a><i class="kirafont icon-time-circle-fill"></i>大概 11 分钟</a>
		</div>
		<!-- toc --><html><head></head><body><ul>
<li><a href="#%E6%9E%B6%E6%9E%84">架构</a></li>
<li><a href="#%E5%B7%A6%E4%BE%A7%E7%BC%96%E7%A0%81%E5%99%A8encoder-%E7%90%86%E8%A7%A3%E8%BE%93%E5%85%A5">左侧：编码器（Encoder）—— “理解输入”</a>
<ul>
<li><a href="#inputs--input-embedding%E8%BE%93%E5%85%A5%E4%B8%8E%E5%B5%8C%E5%85%A5">Inputs &amp; Input Embedding（输入与嵌入）：</a></li>
<li><a href="#positional-encoding%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">Positional Encoding（位置编码）：</a>
<ul>
<li><a href="#%E7%8E%B0%E4%BB%A3%E6%BC%94%E8%BF%9B">现代演进</a></li>
<li><a href="#bev%E7%89%B9%E5%BE%81%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">BEV特征的位置编码</a>
<ul>
<li><a href="#%E6%AD%A3%E5%BC%A6%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%842d%E6%89%A9%E5%B1%95">正弦位置编码的2D扩展</a></li>
<li><a href="#%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">可学习的位置编码</a></li>
<li><a href="#3d%E7%A9%BA%E9%97%B4%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">3D空间位置编码</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B">多头注意力</a>
<ul>
<li><a href="#%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B">缩放点积注意力</a></li>
<li><a href="#%E8%BF%87%E7%A8%8B">过程</a></li>
</ul>
</li>
<li><a href="#add--norm%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96">Add &amp; Norm（残差连接与层归一化）</a>
<ul>
<li><a href="#add-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5--residual-connection">Add (残差连接 / Residual Connection)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%B3%E4%BE%A7%E8%A7%A3%E7%A0%81%E5%99%A8decoder-%E7%94%9F%E6%88%90%E8%BE%93%E5%87%BA">右侧：解码器（Decoder）—— “生成输出”</a>
<ul>
<li><a href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E7%9A%84%E8%BE%93%E5%85%A5%E6%98%AF%E4%BB%80%E4%B9%88">解码器的输入是什么？</a></li>
<li><a href="#masked-multi-head-attention%E5%B8%A6%E6%8E%A9%E7%A0%81%E7%9A%84%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B">Masked Multi-Head Attention（带掩码的多头注意力）</a>
<ul>
<li><a href="#%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8">自动驾驶中的应用</a>
<ul>
<li><a href="#%E7%A9%BA%E9%97%B4%E4%B8%8A%E7%9A%84-maskdeformable-attention-%E5%8F%98%E5%BD%A2%E6%B3%A8%E6%84%8F%E5%8A%9B">空间上的 Mask：Deformable Attention (变形注意力)</a></li>
<li><a href="#%E6%97%B6%E9%97%B4%E4%B8%8A%E7%9A%84mask">时间上的mask</a></li>
<li><a href="#%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%E4%B8%8A%E7%9A%84-maskmae-masked-autoencoders--%E4%B8%BA%E4%BA%86%E8%84%91%E8%A1%A5">训练策略上的 Mask：MAE (Masked Autoencoders) —— 为了“脑补”</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8F%88%E6%98%AF%E4%B8%80%E4%B8%AAmulti-head-attention%E4%BA%A4%E4%BA%92%E6%B3%A8%E6%84%8F%E5%8A%9B--cross-attention">又是一个Multi-Head Attention（交互注意力 / Cross Attention）</a></li>
<li><a href="#%E5%8F%B3%E4%BE%A7%E7%9A%84%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">右侧的前馈神经网络</a></li>
<li><a href="#%E6%9C%80%E5%90%8E%E7%9A%84%E8%BE%93%E5%87%BA">最后的输出</a>
<ul>
<li><a href="#%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B">翻译模型：</a></li>
<li><a href="#%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6">自动驾驶</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->
<h1><span id="%E6%9E%B6%E6%9E%84">架构</span></h1>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="transformer.png" alt="The Transformer- model architecture" class="lazyload"></p>
<h1><span id="%E5%B7%A6%E4%BE%A7%E7%BC%96%E7%A0%81%E5%99%A8encoder-%E7%90%86%E8%A7%A3%E8%BE%93%E5%85%A5">左侧：编码器（Encoder）—— “理解输入”</span></h1>
<p>左半部分负责处理和理解输入的信息（比如翻译任务中的源语言句子）。</p>
<h2><span id="inputs--input-embedding%E8%BE%93%E5%85%A5%E4%B8%8E%E5%B5%8C%E5%85%A5">Inputs &amp; Input Embedding（输入与嵌入）：</span></h2>
<p>模型首先接收输入（Inputs），将文字转换为计算机能理解的向量（Embedding）。这里应该有一个网络，训练之后便能实现嵌入</p>
<h2><span id="positional-encoding%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">Positional Encoding（位置编码）：</span></h2>
<p>《Attention Is All You Need》使用了一套基于正弦和余弦函数的数学公式来生成位置信息。</p>
<p>**</p>
<p>$$\text{最终输入} = \text{词嵌入 (Input Embedding)} + \text{位置编码 (Positional Encoding)}$$
**</p>
<p><strong>位置编码</strong>是一个向量，它的维度（Dimension）和词嵌入的维度（$d_{model}$）是一模一样的（通常是 512）。模型把这两个向量对应位置相加。</p>
<p>**Transformer <strong>使用不同频率的</strong>正弦（Sine）<strong>和</strong>余弦（Cosine）**函数来生成这些向量。</p>
<p>**$pos$ **是词在句子中的位置（比如第 1 个词，第 2 个词…）。 $i$ 是向量维度的索引（比如第 $0, 1, 2, …, 511$ 维）。 $d_{model}$ 是模型的维度（比如 512）。</p>
<p><strong>偶数维度（$2i$）使用正弦函数：</strong></p>
<p>$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$</p>
<p><strong>奇数维度（$2i+1$）使用余弦函数：</strong></p>
<p>$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$</p>
<p>对于每一个位置 $pos$，这 512 个维度的正弦/余弦值组合起来，是独一无二的。模型只要看一眼这个向量的波形，就知道它在句子的哪个位置。</p>
<h3><span id="%E7%8E%B0%E4%BB%A3%E6%BC%94%E8%BF%9B">现代演进</span></h3>
<p>BERT / GPT-2： 使用了<strong>可学习的位置编码（Learnable Positional Embedding）</strong>。简单说就是不给公式，直接初始化一个矩阵，让模型在训练中自己去学每个位置最好的表示向量。</p>
<p>LLaMA / ChatGLM / PaLM： 使用了<strong>旋转位置编码（RoPE - Rotary Positional Embedding）</strong>。这是目前的业界主流。它不是通过“加法”把位置信息加进去，而是通过“旋转”向量的角度来注入位置信息，数学性质更好，对长文本的支持更强。</p>
<h3><span id="bev%E7%89%B9%E5%BE%81%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">BEV特征的位置编码</span></h3>
<h4><span id="%E6%AD%A3%E5%BC%A6%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%842d%E6%89%A9%E5%B1%95">正弦位置编码的2D扩展</span></h4>
<p>原理：假设我们有一个 $H \times W$ 的 BEV 网格。我们把嵌入向量的维度（比如 256 维）一分为二：前 128 维：用来对 X 轴坐标 进行标准的正弦/余弦编码。后 128 维：用来对 Y 轴坐标 进行标准的正弦/余弦编码。</p>
<p>$$PE_{(x, y)} = \text{Concat}(\text{PE}(x), \text{PE}(y))$$</p>
<p>BEVFormer，它在初始化那层 $200 \times 200$ 的格子时，就给每个格子加上了这种 xy 坐标编码，让每个格子知道自己在车身周围的物理位置。</p>
<h4><span id="%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">可学习的位置编码</span></h4>
<p>初始化一个大小为 $(H, W, D)$ 的矩阵（参数），让神经网络在训练过程中**自己去“学”**每个格子应该长什么样。</p>
<p><strong>优点：</strong> 灵活，能适应特定数据集的偏差。</p>
<p><strong>缺点：</strong> 外推性差（换个分辨率或网格大小就需要重新训练）。</p>
<h4><span id="3d%E7%A9%BA%E9%97%B4%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">3D空间位置编码</span></h4>
<p>待深入了解 <strong>PETR (Position Embedding Transformation)</strong></p>
<p><strong>核心逻辑：</strong> 视锥位置编码 (3D Coordinates Generator)
<strong>从图像出发：</strong> 对于摄像头拍到的每一张 2D 图片，我们有每个像素的坐标 $(u, v)$ 。
<strong>反投影：</strong> 利用相机的内参（Intrinsics）和外参（Extrinsics），我们可以把这些 2D 像素通过一条射线射向 3D 空间。
<strong>离散化：</strong> 在这条射线上取不同的深度点，计算出它们在真实世界（车身坐标系）中的 $(x, y, z)$ 坐标。
<strong>编码：</strong> 将这些计算出来的真实 3D 坐标 $(x, y, z)$ 输入到一个简单的神经网络（MLP）中，生成位置编码。
<strong>注入：</strong> 把这个包含了 3D 信息的编码直接加到 2D 图片特征上。</p>
<h2><span id="%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B">多头注意力</span></h2>
<h3><span id="%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B">缩放点积注意力</span></h3>
<p>与临时抱佛脚中提到的QKV相同</p>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
Scaled Dot-Product Attention（缩放点积注意力），它是每个“头”内部的计算方式。</p>
<p>为什么要除以 $\sqrt{d_k}$ ？ ：</p>
<blockquote>
<p>如果不除以这个数，当维度很高时，$Q \cdot K^T$ 的点积结果数值会非常大。</p>
</blockquote>
<h3><span id="%E8%BF%87%E7%A8%8B">过程</span></h3>
<p>Multi-Head Attention 模块为了保证输入输出维度一致（方便堆叠），在最后有一步**融合（Concat + Linear）**的操作。</p>
<ol>
<li>拆分 (Split)：假设模型维度 $d_{model} = 512$，你有 $P=8$ 个头。我们把输入的 512 维向量切成 8 份，每份 64 维。每个头只处理属于自己的那 64 维信息。</li>
<li>独立计算 (Parallel Attention)：这 8 个头并行工作。头 1 计算出它的结果 $Z_1$（维度 64）。…头 8 计算出它的结果 $Z_8$（维度 64）。这里确实产生了 P 个向量（或者说是 P 组结果）。</li>
<li>拼接 (Concatenate) —— 关键一步：模型把这 8 个结果拼起来：</li>
</ol>
<p>$$\text{MultiHead} = \text{Concat}(Z_1, Z_2, …, Z_8)$$
拼完之后，维度又变回了 $64 \times 8 = 512$ 。
4. 线性变换 (Linear Projection)：最后，再通过一个由权重矩阵 $W^O$ 控制的线性层（Linear Layer），把拼好的信息混合一下。</p>
<p>$$\text{Final Output} = \text{Concat}(Z_1, …, Z_P) \cdot W^O$$</p>
<h2><span id="add--norm%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96">Add &amp; Norm（残差连接与层归一化）</span></h2>
<p>$$\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))$$</p>
<h3><span id="add-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5--residual-connection">Add (残差连接 / Residual Connection)</span></h3>
<p><strong>把输入 $x$ 直接加到输出上。</strong></p>
<p><strong>作用：</strong> 解决了“梯度消失”</p>
<p>在深层网络中，计算梯度需要运用链式法则（Chain Rule），也就是无数个导数连乘。</p>
<p>$$\frac{\partial Loss}{\partial x} = \frac{\partial Loss}{\partial y} \cdot \frac{\partial y}{\partial x} \cdot \dots$$
如果中间的导数（$\frac{\partial y}{\partial x}$）都小于 1（通常如此），乘着乘着，梯度就变成 0 了————梯度消失。</p>
<p>有了 Add 的时候：正向公式是：$y = x + F(x)$我们对 $x$ 求导，结果是：</p>
<p>$$\frac{\partial y}{\partial x} = 1 + \frac{\partial F(x)}{\partial x}$$
不会使梯度为0</p>
<h1><span id="%E5%8F%B3%E4%BE%A7%E8%A7%A3%E7%A0%81%E5%99%A8decoder-%E7%94%9F%E6%88%90%E8%BE%93%E5%87%BA">右侧：解码器（Decoder）—— “生成输出”</span></h1>
<h2><span id="%E8%A7%A3%E7%A0%81%E5%99%A8%E7%9A%84%E8%BE%93%E5%85%A5%E6%98%AF%E4%BB%80%E4%B9%88">解码器的输入是什么？</span></h2>
<p>在翻译模型中：截至目前为止，已经翻译出来的词
在端到端模型中：一组固定的、可学习的“查询向量” (Learnable Object Queries)</p>
<h2><span id="masked-multi-head-attention%E5%B8%A6%E6%8E%A9%E7%A0%81%E7%9A%84%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B">Masked Multi-Head Attention（带掩码的多头注意力）</span></h2>
<p><strong>训练时的因果关系:</strong></p>
<p>假设我们要把 “I love you” 翻译成 “我爱你”。 在训练时，我们把正确答案 “我爱你” 作为输入喂给解码器。当模型正在预测第 2 个字（应该是“爱”）时，它只能看到第 1 个字“我”。如果没有 Mask，模型通过注意力机制会偷看到后面的“爱”和“你”。</p>
<p>在计算 $softmax(\frac{QK^T}{\sqrt{d_k}})$ 之前，我们强行把当前位置之后的分数全部变成负无穷大（$-\infty$）。</p>
<h3><span id="%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8">自动驾驶中的应用</span></h3>
<h4><span id="%E7%A9%BA%E9%97%B4%E4%B8%8A%E7%9A%84-maskdeformable-attention-%E5%8F%98%E5%BD%A2%E6%B3%A8%E6%84%8F%E5%8A%9B">空间上的 Mask：Deformable Attention (变形注意力)</span></h4>
<p>隐式 Mask）： 这里的 Mask 不是简单的“遮住”，而是**“只看想看的地方”**。</p>
<p>**传统 Attention： **我（Query）要和全图所有的点（Key）算相似度。</p>
<p>**Deformable Attention（变种 Mask）： **我（Query）只和离我最近的、或者我感兴趣的 4 个点算相似度。其他的点，默认 Mask 掉（权重为 0），根本不参与计算。</p>
<h4><span id="%E6%97%B6%E9%97%B4%E4%B8%8A%E7%9A%84mask">时间上的mask</span></h4>
<p>轨迹预测 (Motion Prediction) —— 为了“不穿越”
当模型在预测 $t=2$ 的位置时，我们必须把 $t=3, 4, 5, 6$ 的真实轨迹 Mask 掉。这使用标准的因果掩码（Causal Mask / Triangular Mask）。</p>
<h4><span id="%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%E4%B8%8A%E7%9A%84-maskmae-masked-autoencoders--%E4%B8%BA%E4%BA%86%E8%84%91%E8%A1%A5">训练策略上的 Mask：MAE (Masked Autoencoders) —— 为了“脑补”</span></h4>
<p>随机遮挡： 把摄像头拍到的画面，或者 BEV 网格，随机抠掉 40% 的块（Mask 掉，设为 0）。</p>
<p>让模型猜： 强迫模型利用剩下的 60% 信息，去还原那被遮住的 40%。</p>
<p>为什么这么做？ 这能逼迫模型理解环境的连续性。 比如：Mask 遮住了车道线的中间一段。模型必须学会：“上面有线，下面也有线，那中间这个黑洞（Mask）里肯定也是线。” 这种 Mask 不是为了生成，而是为了训练模型对环境的深度理解能力。</p>
<p>(⊙﹏⊙)</p>
<p>随后的ADD &amp; NORM不再赘述</p>
<h2><span id="%E5%8F%88%E6%98%AF%E4%B8%80%E4%B8%AAmulti-head-attention%E4%BA%A4%E4%BA%92%E6%B3%A8%E6%84%8F%E5%8A%9B--cross-attention">又是一个Multi-Head Attention（交互注意力 / Cross Attention）</span></h2>
<p><strong>输入来源：</strong></p>
<p><strong>Query (Q) 来自哪里？</strong> 来自Decoder（下方，也就是刚刚经过 Masked Attention 处理过的信息）。
<strong>Key (K) 和 Value (V) 来自哪里？</strong> 来自Encoder（左边，也就是对源句子的完整理解）。</p>
<p>Cross Attention 的输出，本质上是“被注入了新信息的 Query”。</p>
<p>如端到端自动驾驶中为
输出 = “抓取到了物体特征的 Query”
不过还不是坐标框 (x, y, w, h)。它只是一串包含了物体信息的数学特征。</p>
<h2><span id="%E5%8F%B3%E4%BE%A7%E7%9A%84%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">右侧的前馈神经网络</span></h2>
<p>根据attention去生成包含规划、决策信息的高维向量。大概这样粗糙理解吧</p>
<h2><span id="%E6%9C%80%E5%90%8E%E7%9A%84%E8%BE%93%E5%87%BA">最后的输出</span></h2>
<p>端到端自动驾驶与图中的翻译模型不同</p>
<h3><span id="%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B">翻译模型：</span></h3>
<p><strong>Linear 的作用：拉伸（Mapping）</strong></p>
<ul>
<li>Decoder 输出来的向量只有 512 维。但是我们词表里有 30,000 个单词。
所以这个 Linear 层的作用是：把 512 维的特征，强行拉伸成 30,000 维 的超长向量。每一维对应一个单词的打分（Logits）。</li>
</ul>
<p><strong>Softmax 的作用：归一化（Probability）</strong></p>
<ul>
<li>Linear 输出的分数可能是 [10.5, -3.2, 55.0, …]，这没法看。Softmax 把它们变成概率（加起来等于 1）：[0.1, 0.0, 0.9, …]。意思就是：有 90% 的概率是单词 C，有 10% 的概率是单词 A。</li>
</ul>
<h3><span id="%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6">自动驾驶</span></h3>
<p>要预测的是轨迹坐标 $(x, y)$，这是一个回归任务（Regression），而不是分类任务。</p>
<p>结构： 通常是 MLP（即 Linear -&gt; ReLU -&gt; Linear）。</p>
<p>为什么用 MLP？ 因为轨迹是很复杂的曲线，一层 Linear 往往拟合得不够准，多加一层 ReLU 能让它画出更顺滑的弯道。</p>
<p>没有softmax</p>
<!-- hexo injector body_end start -->
<!-- Mermaid Scripts -->
<script>
// 检查页面是否包含Mermaid内容
const hasMermaid = document.querySelector('.mermaid') !== null;

// 如果存在Mermaid图表，则加载Mermaid库
if (hasMermaid) {
  // 加载Mermaid库
  const mermaidScript = document.createElement('script');
  mermaidScript.src = 'https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js';
  mermaidScript.onload = function() {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      securityLevel: 'loose',
      fontFamily: 'inherit'
    });

    // 重新渲染Mermaid图表
    mermaid.init(undefined, '.mermaid');
  };
  document.head.appendChild(mermaidScript);
}
</script><!-- hexo injector body_end end --></body></html>
	</article>

	 
    <div class="kira-post-copyright">
        <strong>本文作者：</strong>战斗包子<br>
        <strong>本文链接：</strong><a href="https://paipai121.github.io/2025/11/23/%E5%B7%A5%E4%BD%9C/%E8%8D%89%E5%B1%A5%E8%99%AB%E7%9A%84transformer%E5%88%9D%E6%AD%A5%E7%90%86%E8%A7%A3/" title="https:&#x2F;&#x2F;paipai121.github.io&#x2F;2025&#x2F;11&#x2F;23&#x2F;工作&#x2F;草履虫的transformer初步理解&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;paipai121.github.io&#x2F;2025&#x2F;11&#x2F;23&#x2F;工作&#x2F;草履虫的transformer初步理解&#x2F;</a><br>
        
            <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
        
    </div>

  
	<div class="kira-post-nav">
		<nav class="post-nav">
			
		</nav>
	</div>
	
	<div class="kira-post-meta kira-rainbow">
		
		
			<a class="kirafont icon-tag-fill -none-link" href="/tags/AI/" rel="tag">AI</a> <a class="kirafont icon-tag-fill -none-link" href="/tags/%E5%AD%A6%E4%B9%A0/" rel="tag">学习</a> <a class="kirafont icon-tag-fill -none-link" href="/tags/%E8%8D%89%E5%B1%A5%E8%99%AB%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF/" rel="tag">草履虫的端到端</a>
		
	</div>
	
	<div class="kira-post-footer">
		

		
	<div class="giscus"></div>
  
    <script src="https://giscus.app/client.js"
      data-repo="PaiPai121/discuss"
      data-repo-id="R_kgDOMFuZdw"
      data-category="Announcements"
      data-category-id="DIC_kwDOMFuZd84Cf5yz"
      data-mapping="pathname"
      data-strict="0"
      data-reactions-enabled="1"
      data-emit-metadata="0"
      data-input-position="top"
      data-theme="preferred_color_scheme"
      data-lang="zh-CN"
      data-loading="lazy"
      crossorigin="anonymous"
      async  
    ></script>
  

	</div>
	
</div>

				</div>
			</div>
			<div class="kira-right-column">
	<a onclick="document.querySelector('#kira-top-header').scrollIntoView({behavior: 'smooth'});" class="kira-backtotop" aria-label="回到顶部" title="回到顶部">
		<button class="mdui-fab mdui-ripple">
			<i class="kirafont icon-caret-up"></i>
		</button>
	</a>
</div>

		</div>
	</body>
</html>
